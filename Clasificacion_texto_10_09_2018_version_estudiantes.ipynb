{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![HEADER](IMAGENES/Seleccion_859.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![d1](IMAGENES/Seleccion_861.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![HEADER](IMAGENES/Seleccion_860.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![HEADER](IMAGENES/Seleccion_862.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# CLASIFICACIÓN DE TEXTO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Los algoritmos de **_clasificion de texto_** tienen una amplia aplicacion en el sector industrial, que va desde filtar spams hasta analizar sentimientos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "![spams](IMAGENES/Seleccion_822.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hideOutput": true,
    "hidePrompt": false
   },
   "source": [
    "## Los pasos para abordar este tipo de problemas son:\n",
    "![pasos](IMAGENES/Seleccion_823.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideOutput": true
   },
   "source": [
    "![HEADER](IMAGENES/Seleccion_859.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algunos modelos de representación de texto\n",
    "\n",
    "* Countvertorizer\n",
    "* Tfidf\n",
    "* Hashing\n",
    "* N grams\n",
    "* Word2vec\n",
    "* Word embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algunos modelos de ML usados para problemas de clasificación de texto\n",
    "\n",
    "* SVM\n",
    "* Arboles de decisión\n",
    "* Métodos de ensamble\n",
    "* Naive Bayes\n",
    "* Redes neuronales\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seleccionar un modelo de ML para clasificación de texto\n",
    "__[Enlace Google developer](https://developers.google.com/machine-learning/guides/text-classification/step-2-5)__\n",
    "\n",
    "![pasos](IMAGENES/TextClassificationFlowchart.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![HEADER](IMAGENES/Seleccion_859.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo Bag of Word\n",
    "\n",
    "El modelo bag of words es una representación de un documento sin orden, solo importa el conteo de las palabras. Como alternativa a este inconveniente del BoW se pueden usar los N gramas, sin embargo, en problemas de clasificación de texto el modelo BoW obtiene resultados aceptables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![BoW](IMAGENES/Seleccion_838.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notas del BoW\n",
    "\n",
    "> * Es una forma común de representar documentos en forma de matriz.\n",
    "> * Construye una matriz de N x T\n",
    "> * Cada columna representa un término único, y cada celda i, j representa cuántos del término j están en el documento i.\n",
    "> * El orden de las palabras dentro de un documento no se toma en cuenta en el modelo\n",
    "> * Una vez que tenemos nuestra matriz de término de documento, podemos usar técnicas de ML\n",
    "> * Es un bloque fundamental para muchas técnicas más avanzadas.\n",
    "> * Lo que estamos haciendo es extraer información potencialmente relevante de una manera que la computadora puede utilizar (es decir, números)\n",
    "> * Es buena practica minimizar la distancia entre dos vectores de características similares\n",
    "> * Se debe procurar no perder mucha informacion, por ejemplo cuando se convierten a minusculas nombre propios Apple y apple o cuando se lematiza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![HEADER](IMAGENES/Seleccion_859.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TERM FREQUENCY (TF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contamos la cantidad de veces que aparece cada término en cada documento y esta cantidad de veces que un término ocurre en un documento se denomina Term frequency (TF).\n",
    "**El peso de un término que aparece en un documento es simplemente proporcional al término frecuencia**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INVERSE DOCUMENT FREQUENCY (IDF)\n",
    "\n",
    "Debido a que por ejemplo el término \"the\" es muy común, TERM FREQUENCY tenderá a enfatizar incorrectamente los documentos que utilizan la palabra \"the\" con mayor frecuencia, sin dar suficiente importancia a los términos más significativos (con más información).\n",
    "Por lo tanto, se incorpora INVERSE DOCUMENT FREQUENCY que disminuye el peso de los términos que ocurren con mucha frecuencia en la colección de documentos y aumenta el peso de los términos que ocurren con menos frecuencia.\n",
    "La especificidad de un término se puede cuantificar como una función inversa del número de documentos en que se produce.\n",
    "\n",
    "\n",
    "![ecuacion1](IMAGENES/Seleccion_863.png)\n",
    "\n",
    "N: Numero total de documentos <br>\n",
    "nt: Numero de documentos donde el termino t aparece.\n",
    "\n",
    "* ¿Qué pasa si el termino no aparece en la colección de documentos? \n",
    "* Esto conduciría a división por cero, es por esto que se usa la siguiente formula frecuentemente (más adelante veremos que SKLEARN usa una formula diferente)\n",
    "\n",
    "![ecuacion1](IMAGENES/Seleccion_870.png)\n",
    "\n",
    "![ecuacion1](IMAGENES/Seleccion_871.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "y = [-np.log(x/100) for x in range(1,100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0xc0debb0>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWsAAAD4CAYAAAAqw8chAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAdqklEQVR4nO3deXycdaHv8c8ve5OZ7JOkTbO0SboRSPeFFqmAWBEEj4osol45VrmgnHP0eI/ec68vz/VsXvW4cA5aQcWteEGEgkVASmkRuqR7m3RN0zbNnjR72my/+8dMY1eatpk8z8x836/XvCYzmWa+T5/2+/rlN7/neYy1FhERcbcopwOIiMilqaxFREKAylpEJASorEVEQoDKWkQkBMQE44dmZmbawsLCYPxoEZGwtGXLlmZrre9i3w9KWRcWFlJeXh6MHy0iEpaMMUfe7fuaBhERCQEqaxGREKCyFhEJASprEZEQoLIWEQkBKmsRkRCgshYRCQGuKeuhIctjaw6wbn+T01FERFzHNWUdFWX48boqXq9scDqKiIjruKasAXKSE2joOOV0DBER13FVWWcnJ1DfcdLpGCIiruO6sm5QWYuInMdVZZ2TEk9j5ymGhnRdSBGRM7mqrLOTExgcsjR3a95aRORMritrgIZ2lbWIyJlcVdY5p8ta89YiImdxVVmfHllrRYiIyNlcVdaZnjiiDDSqrEVEzuKqso6JjsLnjdfIWkTkHK4qazh9YIw+YBQROZMry1rTICIiZ3NdWefokHMRkfO4rqyzk+Np6+nnZP+g01FERFzDhWXtX77XqHlrEZFhrivrnBSttRYROZfryloHxoiInM+1Za0VISIif+G6sk5OiGFcbDT17SprEZHTXFfWxhiyk3UUo4jImVxX1nD6wBitBhEROc2VZZ2TogNjRETONOKyNsZEG2O2GWNeCmYg+MuFc63V5b1ERODyRtaPApXBCnKm7OQE+gaGaO/tH4u3ExFxvRGVtTFmIvBB4IngxvHL0VprEZGzjHRk/T3gK8DQxV5gjFlujCk3xpQ3NTVdVajs5HgALd8TEQm4ZFkbY24HGq21W97tddbaFdbaudbauT6f76pC6fwgIiJnG8nIejHwIWNMNfA0cJMx5lfBDJV1emStaRAREWAEZW2t/aq1dqK1thC4B1hjrf1EMEPFx0STnhSnshYRCXDlOmuAvLRxHGnpdjqGiIgrXFZZW2vXWmtvD1aYM03LSaayrlNrrUVEcPHIetp4L63dfTR16UNGERHXlvXUHC8A++o7HU4iIuI815b1tJxkAPbWqaxFRFxb1ulJcWR549mrkbWIiHvLGmDa+GT21nc4HUNExHHuLuscLwcauxgYvOhR7iIiEcH1Zd03MES11luLSIRzdVmfXhFSqQ8ZRSTCubqsi7M8REcZLd8TkYjn6rKOj4mmyJekDxlFJOK5uqwBpgYOOxcRiWSuL+tpOV6Ot/XScVKX+BKRyBUSZQ2wX/PWIhLB3F/W4wOHnausRSSCub6sJ6Qk4E2I0YeMIhLRXF/Wxhim5ySz+7jKWkQil+vLGmBuYRq7j7fTfWrA6SgiIo4IibJeVJTBwJCl/MgJp6OIiDgiJMp6TkEaMVGGDVUtTkcREXFESJR1YlwMZXmpKmsRiVghUdYAiyZnsLOmnS7NW4tIBAqZsl44OYPBIUt5davTUURExlzIlPXsglRiow0bqlTWIhJ5QqasE+NiKJuYyjuatxaRCBQyZQ3+JXy7j7fTqZM6iUiECamyHp631nprEYkwIVXWs/PTAvPWmgoRkcgSUmU9Li6aWXlpvHNIZS0ikSWkyhrgxqk+dta0U99+0ukoIiJjJuTK+v3X5ADwyp56h5OIiIydkCvr4iwPJVkeXt5d53QUEZExE3JlDfCB0hw2HW6lpeuU01FERMZESJb1stLxDFl4taLB6SgiImMiJMt6+ngvBRmJ/HG35q1FJDJcsqyNMQnGmE3GmB3GmD3GmG+MRbBLZGLZNTm8faiZ9l4dzSgi4W8kI+tTwE3W2jJgJrDMGLMwuLEubVlpDv2DltcrNRUiIuHvkmVt/boCD2MDNxvUVCNQNjGV8SkJvKypEBGJACOaszbGRBtjtgONwGvW2o0XeM1yY0y5Maa8qalptHOeJyrKsKw0hzf3N9Heo6kQEQlvIypra+2gtXYmMBGYb4wpvcBrVlhr51pr5/p8vtHOeUEfnTORvoEhfr+tZkzeT0TEKZe1GsRa2wasBZYFJc1lumZCCtdNTOHpzcew1vGZGRGRoBnJahCfMSY18PU44BZgb7CDjdS98/PZW9/JtmNtTkcREQmakYysxwNvGGN2Apvxz1m/FNxYI3dH2QQS46J5etNRp6OIiATNSFaD7LTWzrLWXmetLbXW/tNYBBspT3wMHyqbwIs76nQFGREJWyF5BOO57p2fT2//IC9sr3U6iohIUIRFWV83MYXp45NZqakQEQlTYVHWxhjuW5DPntoONh1udTqOiMioC4uyBvjo7ImkJ8XxozcPOR1FRGTUhU1Zj4uL5tPXF7JmbyN76zucjiMiMqrCpqwBPrmogMS4aH78ZpXTUURERlVYlXVqYhz3zc9n1Y5ajrX2OB1HRGTUhFVZAzx4wySiDDyxXqNrEQkfYVfW41PG8eFZuTy9+RhNnbpGo4iEh7Ara4CHlhYzMGR5bM0Bp6OIiIyKsCzrSZlJ3DMvj19vPEp1c7fTcURErlpYljXAozeXEBsdxbdf3ed0FBGRqxa2ZZ2VnMBnb5jESzvr2Fmj06eKSGgL27IG+Ox7JpOeFMe/vbxXFycQkZAW1mXtTYjlCzcV8/ahFtbsbXQ6jojIFQvrsga4f0EBxVkevr5qD719g07HERG5ImFf1nExUfyfO0upOdHLY29oKZ+IhKawL2uARUUZ/NWsXFasq+JgY6fTcURELltElDXA1z44nXGx0fzj87v1YaOIhJyIKetMTzxfWTaNDVWtPLulxuk4IiKXJWLKGuC++fnMK0zjn16s4Hhbr9NxRERGLKLKOirK8J2PzWTQWr7y7A6GhjQdIiKhIaLKGiA/I5F//OAM/nywhV9uOOJ0HBGREYm4sga4d34eS6f6+NeXKznU1OV0HBGRS4rIsjbG8K2PXEdCbDRf+M02TvbrYBkRcbeILGvwn+jpu3eXUVHXwTderHA6jojIu4rYsga4aVo2n7+xiJWbjvL8tuNOxxERuaiILmuAL986hfmF6Xzt97t0dKOIuFbEl3VMdBQ/uHcW42KjWf6LLbT39DsdSUTkPBFf1gA5KQk8/ok5HDvRwyMrtzIwOOR0JBGRs6isA+ZPSuef77qW9Qea+eYfKp2OIyJylhinA7jJ3fPy2N/QyRNvHaY4y8MnFhY4HUlEBFBZn+ert02nqrmb//3CbrK88dx6TY7TkURENA1yrugow2P3zeLaial8YeU2yqtbnY4kInLpsjbG5Blj3jDGVBpj9hhjHh2LYE5KjIvhZ5+eR27qOB58qpz9DVrSJyLOGsnIegD4krV2OrAQeNgYMyO4sZyXnhTHU5+ZT3xMFA88uZEjLd1ORxKRCHbJsrbW1llrtwa+7gQqgdxgB3ODvPREfvngAvoGhrjvJxs51trjdCQRiVCXNWdtjCkEZgEbL/C95caYcmNMeVNT0+ikc4GpOV5++eACOk/2c98TG6jVRQtExAEjLmtjjAf4HfA31tqOc79vrV1hrZ1rrZ3r8/lGM6PjSnNT+OWDC2jr7ue+n2zQVWZEZMyNqKyNMbH4i/rX1trnghvJncryUnnqwfm0dPdx94/eobpZc9giMnZGshrEAE8Cldba7wY/knvNzk9j5WcX0ts/yN0/focDWiUiImNkJCPrxcADwE3GmO2B221BzuVapbkp/Hb5QgDu/vE7bDt6wuFEIhIJRrIa5C1rrbHWXmetnRm4rR6LcG5Vku3lmc8vInlcLPf9ZCNr9jY4HUlEwpyOYLxCBRlJPPv56ynO8vDZX2zht5uPOh1JRMKYyvoq+LzxPL18IYuLM/kfv9vFt/64l6Eh63QsEQlDKuurlBQfw5Ofmsu98/P5r7WHeOjXW+jpG3A6loiEGZX1KIiNjuJfPlzK/7p9Bq9VNPCxH72jtdgiMqpU1qPEGMODSybx5KfmcbSlhzt++BZvH2p2OpaIhAmV9Sh777Qsnn9kMelJcTzw5CaeWF+FtZrHFpGro7IOgiKfh+cfXsytM7L55h8qeehXW2nv1YV4ReTKqayDxBMfw3/dP5uv3TaNP1U2cPsP17Ozps3pWCISolTWQWSMYfl7ivjt5xYxOGj5yONv88T6Ki3vE5HLprIeA3MK0lj96A0snZrFN/9QySd/uomGjpNOxxKREKKyHiOpiXGseGAO//Lhayk/0sr7v7eOl3fVOR1LREKEynoMGWO4b0E+L33hBvLSEnno11v54spttPX0OR1NRFxOZe2A4iwPz/336/nbW6awelcd7/uPdfypQieDEpGLU1k7JDY6ikdvKeH5hxeTkRTHX/+inEd+s5XmrlNORxMRF1JZO6w0N4VVjyzh7943hVf3NHDLd9/kmfJjOpBGRM6isnaBuJgovnhzCasfXUKRz8PfP7uTe1Zs4GCjrkQjIn4qaxcpzvLyzOcW8a9/dS176zv5wPfX8+9/3Kuz+ImIytptoqIM987PZ82XbuRDZbk8vvYQN337TVbtqNXUiEgEU1m7VIYnnu/cXcbvHlpEhieOL67cxsdXbGD38Xano4mIA1TWLjenIJ1Vjyzhnz9cysHGLu547C2+/MwOHQEpEmFU1iEgOspw/4IC1v79UpbfMJlV22tZ+n/X8t1X99F1SvPZIpFAZR1CkhNi+ept0/nT393IzdOz+MGag9z4rTd46u1q+gaGnI4nIkGksg5B+RmJPHbfbF54eDEl2R6+vmoPN31nLc9uqWFQZ/QTCUsq6xBWlpfKys8u5Of/bR6pibF8+ZkdvP9763hpZ61OwyoSZlTWIc4Yw9KpWbz4yBIev382AI/8Zhsf+P56Vu+qU2mLhAkTjLW7c+fOteXl5aP+c+XSBocsL+2s5QevH+BQUzdTsj08/N5ibr9uAtFRxul4InIRxpgt1tq5F/2+yjo8nS7tx9Yc5EBjF5Myk3joxiLumpVLXIx+oRJxG5V1hBsasrxaUc8P1xxkT20HOckJ/PUNk7h3fj5J8TFOxxORAJW1AGCtZd2BZh5fe5ANVa0kJ8TwiYUFfPr6QrKSE5yOJxLxVNZynm1HT7BiXRV/3FNPTJThzpm5fGbxJGZMSHY6mkjEUlnLRR1p6eaJ9Yd5dksNvf2DLJqcwWeWTOKmaVn6MFJkjKms5ZLae/pZufkoT71dTV37SfLSx/HAwgI+PjeflMRYp+OJRASVtYzYwOAQr1Y08PM/V7OpupWE2CjuLMvlgUUFlOamOB1PJKyprOWK7Klt51cbjvD8tlp6+wcpy0vl/vn53F42nsQ4rSIRGW1XXdbGmJ8CtwON1trSkbypyjp8tPf289zWGn614QiHmrrxxsdw16xc7pmfxzUTNNoWGS2jUdbvAbqAX6isI5e1ls3VJ/jNxiOs3l1P38AQ1+am8PF5edxRNoGUcZrbFrkaozINYowpBF5SWQtAW08fz287ztObj7G3vpP4mCiWlebwsTl5XF+UQZRWkohctjEra2PMcmA5QH5+/pwjR45cdlgJLdZadh1v55nyGl7YfpyOkwNMSEngrlm5fGTORIp8HqcjioQMjaxlTJzsH+TVigae21rDuv1NDFkom5jCXbNyuaNsApmeeKcjiriaylrGXGPHSV7YXsvvtx2noq6D6CjDDSWZfKhsArdek4NH5yQROY/KWhy1r76T57cfZ9X2Wo639RIfE8Ut07O5o2w8S6dmkRAb7XREEVcYjdUgK4GlQCbQAHzdWvvku/0ZlbWca2jIsvXoCV7YXsvLu+to7urDEx/DLdOz+OB1E7ihJFPFLRFNB8WI6wwMDrGhqpUXd9TySkU9bT39w8W9rHQ8S6f6VNwScVTW4mr9g0O8faiF1Tvrhos7MS6a907N4v2lObx3qg9vgtZwS/hTWUvI6B8cYmNVK6t31/Hqnnqau/qIi45icXEGt16Tw83Ts8jy6tzbEp5U1hKSBgNz3K/srueVinqOtfZiDMzKS+WWGdncOiObIp8HY3QAjoQHlbWEPGst+xo6eXVPA69W1LP7eAcABRmJ3Dwtm5unZzGvMF3XlpSQprKWsFPX3svrlY28VtHAO1Ut9A0M4YmP4YaSTN47LYulU32aLpGQo7KWsNZ9aoA/H2xmzd5G3tjXSEPHKQCuzU1h6VQfS6f6mJmXpivfiOuprCViWGupqOvgjb2NrN3XxNajJxiykDIuliXFmdw4xcd7pvjISdGoW9xHZS0Rq72nn/UHm3hzXxPrDjQNj7qnZHu4ocTHkpJMFkxK18UUxBVU1iL4R9176ztZf6CJ9Qea2Xi4lb6BIeKio5hdkMqS4kwWF2dybW4KMdH6oFLGnspa5AJO9g+yubqVtw40s/5AMxV1/hUm3oQYFk7OYHFRBtcXZ1KSpeWBMjZU1iIj0NJ1ircPtfD2oWbeOtjMsdZeADI9cSycnMGiogwWTc5gUmaSyluCQmUtcgWOtfbwTqC836lqGZ7vzk6OZ+HkDBZOzmDBpHSVt4yaS5W1PlkRuYC89ETy0hO5e14e1loON3fz9qEWNh5u5e1DLbywvRYAnzee+ZPSWTApnfmT0pmS5dVlzSQoVNYil2CMYbLPw2Sfh08sLBgu742HW9lY5S/wP+ysA/zLBOcVpjG3MJ15helcm5uiIytlVKisRS7TmeV97/x8rLXUnOhl0+FWNh1uZXN1K3+qbAQgPiaKsrxU5hakMa8wndn5aaQk6iyCcvk0Zy0SBE2dpyivbqX8yAnKq1vZU9vBwJD//1pJloc5BWnMLkhjdn4aRT7Ne4s+YBRxhZ6+AXYca2fLEX+BbzvaRntvP+CfOpmVn8rsfH95l+Wl6BzeEUgfMIq4QGJcjH/5X1EG4L/MWVVzF1uPtLHlyAm2Hj3B2n1NABjjH33PyktjZn4qZRNTmZLt0cE6EU4jaxGXaO/tZ/uxNrYfbWP7sRNsO9ZGW49/9J0YF01pbgoz8/zlXZaXQm7qOE2fhBGNrEVCRMq4WG6c4uPGKT7Af4j8kZYef4EHbj//czV9g0MAZCTFcd3EFK4LlPd1E1PJ9MQ7uQkSRCprEZcyxlCYmURhZhJ3zcoFoG9giMq6DnbWtLGjpp0dx9pYu7+J078gT0hJ4NpAgZfmpnBtbgrpSXEOboWMFpW1SAiJCywFLMtL5YHAc92nBthT6y/wXcfb2VXTzit7Gob/TG7qOEpzkymdkEJpbgrX5Cbr4gwhSGUtEuKS4mOYHziC8rT23n721Laz+3g7u453sPv42QWe5Y33F/eE5MAthYlpmgN3M5W1SBhKGRfL9UWZXF+UOfxc58l+Kmo72HW8nYraDvbUdvDm/iYGA+u/kxNimD7eX9wzJiQzfbyXkiyvjsB0CZW1SITwJsSyYHIGCyZnDD93sn+QvfWd7KltZ09tB5V1HazcdJTe/kEAYqMNRT4PM8YnM3345iVDH2SOOZW1SARLiI1mZl4qM/NSh58bHPKf+6SyroOKug4qajt462Azz207PvwanzfeX9w5XqaN9zI1O5niLI9G4UGkshaRs0RHGYqzPBRnebijbMLw8y1dp6is62RvfQeVdZ1U1nXws0Mtw0sJY6IMk31JTM1JZlqOl6nZXqbmeMlNHaczEY4ClbWIjEiGJ54lJfEsKfnLPHj/4BDVzd1U1neyr76DffWdbDt6ghd31A6/JikumpJsf3lPyfEyJdvDlGwvWd54faB5GVTWInLFYqOjKMn2UpLthTNG4Z0n+9nf0MW+QInvb+jitcoGflt+bPg1KeNimZLtoSTby5Qsf4GXZHvJ9MSpxC9AZS0io86bEMucgjTmFKSd9Xxz1yn2N3Syv76TfQ1dHGjo5KUdtXScHBh+TWpiLCVZHoqzvJRkeSjJ9k/J5CQnRHSJq6xFZMxkeuLJ9MSftaTQWktj5ykONHRxoLGT/Q1dHGzs5OXddawMnBsFwBMfQ5EviaLAfHqxz3+fn54YESe5UlmLiKOMMWQnJ5CdnHDWfLi1luauPg42dnGwqYuDDZ0cbOrizwebeW7rX1amxEYbCjOSKPJ5KMry3/svDpFEchidalZlLSKuZIzB543H540fPrXsaR0n+6lq6vYXeWMXh5q62N/YyWuVDcMH+YB/iWGRL8lf3pmnizyJ3NRxITcaV1mLSMhJTog9b304+E90dbS1h0NNXVQ1dQfuu1i9q274dLPgH40XZCQxKTOJyZn++0mZSUzyJeHzuHOVyojK2hizDPg+EA08Ya39t6CmEhG5AnExUcNrxM/V2t1H1ekSb+6iurmbqqZu3tzXNLxWHPxz44WZiRRm+Iu88IwyT0107gyGlyxrY0w08J/A+4AaYLMxZpW1tiLY4URERkt6UhzpSenMLUw/6/nBIUttWy9Vzd0cbuqiuqWHquZudtS0sXpXHWfMqpAyLtZf3hmJFGQkDZd6YUYSqYmxQR2Rj2RkPR84aK2tAjDGPA3cCaisRSTkRUcZ8tITyUtPHL7ww2mnBgY51tpLdXM31S3dHA7cb64+wQs7ajnzQlvJCTFMzfHy/z63KCilPZKyzgWOnfG4Blhw7ouMMcuB5QD5+fmjEk5ExEnxMdEXnVbxF3kPR1p6qG7p4UhLN30DQ0EbXY+krC/0zudduNFauwJYAf5rMF5lLhERV/MXuZfiLO+YvN9I1q7UAHlnPJ4I1F7ktSIiEgQjKevNQIkxZpIxJg64B1gV3FgiInKmS06DWGsHjDGPAK/gX7r3U2vtnqAnExGRYSNaZ22tXQ2sDnIWERG5iNA63lJEJEKprEVEQoDKWkQkBKisRURCgLF29I9fMcY0AUeu8I9nAs2jGCdUaLsji7Y7soxkuwustb6LfTMoZX01jDHl1tq5TucYa9ruyKLtjiyjsd2aBhERCQEqaxGREODGsl7hdACHaLsji7Y7slz1drtuzlpERM7nxpG1iIicQ2UtIhICXFPWxphlxph9xpiDxph/cDpPsBhj8owxbxhjKo0xe4wxjwaeTzfGvGaMORC4T3M6azAYY6KNMduMMS8FHk8yxmwMbPdvA6fhDTvGmFRjzLPGmL2Bfb8oEva5MeZvA//OdxtjVhpjEsJxnxtjfmqMaTTG7D7juQvuX+P3g0DX7TTGzB7Je7iirM+4KO8HgBnAvcaYGc6mCpoB4EvW2unAQuDhwLb+A/C6tbYEeD3wOBw9ClSe8fjfgf8IbPcJ4EFHUgXf94E/WmunAWX4/w7Cep8bY3KBLwJzrbWl+E+xfA/huc9/Diw757mL7d8PACWB23Lg8ZG8gSvKmjMuymut7QNOX5Q37Fhr66y1WwNfd+L/T5uLf3ufCrzsKeAuZxIGjzFmIvBB4InAYwPcBDwbeEm4bncy8B7gSQBrbZ+1to0I2Of4T8M8zhgTAyQCdYThPrfWrgNaz3n6Yvv3TuAX1m8DkGqMGX+p93BLWV/oory5DmUZM8aYQmAWsBHIttbWgb/QgSznkgXN94CvAEOBxxlAm7V2IPA4XPf7ZKAJ+FlgCugJY0wSYb7PrbXHgW8DR/GXdDuwhcjY53Dx/XtFfeeWsh7RRXnDiTHGA/wO+BtrbYfTeYLNGHM70Git3XLm0xd4aTju9xhgNvC4tXYW0E2YTXlcSGCO9k5gEjABSMI/BXCucNzn7+aK/t27pawj6qK8xphY/EX9a2vtc4GnG07/KhS4b3QqX5AsBj5kjKnGP811E/6RdmrgV2QI3/1eA9RYazcGHj+Lv7zDfZ/fAhy21jZZa/uB54DriYx9Dhffv1fUd24p64i5KG9gnvZJoNJa+90zvrUK+FTg608BL4x1tmCy1n7VWjvRWluIf/+usdbeD7wBfDTwsrDbbgBrbT1wzBgzNfDUzUAFYb7P8U9/LDTGJAb+3Z/e7rDf5wEX27+rgE8GVoUsBNpPT5e8K2utK27AbcB+4BDwP53OE8TtXIL/V56dwPbA7Tb887evAwcC9+lOZw3i38FS4KXA15OBTcBB4Bkg3ul8QdrmmUB5YL8/D6RFwj4HvgHsBXYDvwTiw3GfAyvxz8v34x85P3ix/Yt/GuQ/A123C/9qmUu+hw43FxEJAW6ZBhERkXehshYRCQEqaxGREKCyFhEJASprEZEQoLIWEQkBKmsRkRDw/wHf7Z5aggCptgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![HEADER](IMAGENES/Seleccion_859.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El juego del ahorcado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![shanno](IMAGENES/Seleccion_845.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![HEADER](IMAGENES/Seleccion_846.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![HEADER](IMAGENES/Seleccion_847.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![HEADER](IMAGENES/Seleccion_848.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![HEADER](IMAGENES/Seleccion_849.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term frequency – Inverse document frequency  (TFIDF)\n",
    "\n",
    "Es un estadístico que pretende reflejar la importancia de una palabra en una colección de documentos. Es uno de los esquemas de ponderación de términos más populares en la actualidad. Se puede utilizar con éxito para el filtrado de BoW en diversos campos como sintesis automáticas y la clasificación del texto.\n",
    "\n",
    "Es el producto de **_TERM FREQUENCY (TF)_** e **_INVERSE DOCUMENT FREQUENCY (IDF)_**\n",
    "\n",
    "* Asigna pesos a las palabras. Las palabras más comunes a lo largo de todos los documentos tienen un peso menor y las palabras menos comunes (en todos los documentos) tienen un peso mayor.\n",
    "* Las menos comunes se consideran que tienen más información.\n",
    "\n",
    "![ecuacion2](IMAGENES/Seleccion_864.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota:**\n",
    "En SKLEARN los ajustes por defecto son TfidfTransformer(norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False).\n",
    "\n",
    "![ecu0](IMAGENES/Seleccion_867.png)\n",
    "\n",
    "* El tf(t,d) es el número de veces que un termino (t) ocurre en un documento dado.\n",
    "* idf(t),es calculado como:\n",
    "\n",
    "![ecu1](IMAGENES/Seleccion_868.png)\n",
    "\n",
    "donde:\n",
    "* nd es el número total de documentos.\n",
    "* df(d,t) es el número de documentos que contienen el termino t.\n",
    "\n",
    "Los vectores tf-idf resultantes son normalizados por la norma euclidiana (I2):\n",
    "\n",
    "\n",
    "![ecu2](IMAGENES/Seleccion_869.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![HEADER](IMAGENES/Seleccion_859.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduccion a las expresiones regulares y tokenizacion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es un lenguaje con su propia sintaxis (ver REGEX), permite hacer match de\n",
    "patrones con otros strings, algunas aplicaciones podrian ser:\n",
    "    * Encontrar paginas web en un documento\n",
    "    * Analizar direcciones de correo electronico, remover o reemplazar caracteres\n",
    "    no deseados\n",
    "\n",
    "Patrones comunes en REGEX\n",
    "\n",
    "Patron\n",
    "* \\w    -> caracter\n",
    "* \\d    -> digit\n",
    "* \\s    -> space\n",
    "* .*    -> comodin\n",
    "* (+)     -> repeticiones\n",
    "* ???   -> Palabra?\n",
    "      \n",
    "Si usamos las letras mayusculas lo que hacemos es negar los patrones\n",
    "* \\S   -> No espacio\n",
    " \n",
    "Tambien podemos hacer grupos metiendolos entre []\n",
    "* [a-z] ->letras minusculas\n",
    "\n",
    "Vamos a ver los metodos mas populares de re\n",
    "\n",
    "* split\n",
    "* findall\n",
    "* seach\n",
    "* match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por qué necesitamos de esto?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo de un tweet\n",
    "\"@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D #boringlife\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![HEADER](IMAGENES/Seleccion_859.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 4), match='abcd'>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.match('abcd','abcdef')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.search('abc','abcdef')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cual es el patron para obtener la siguiente salida?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 0), match=''>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.match('\\???','Bienvenidos al juego')\n",
    "\n",
    "\n",
    "#Patrones\n",
    "\n",
    "# \\w -> caracter\n",
    "# \\d -> digit\n",
    "# \\s -> space\n",
    "# .* -> comodin\n",
    "# (+) -> repeticiones\n",
    "# ??? -> Palabra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Split', 'en', 'espacios']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ejemplo de split string y de re:\n",
    "texto='Split            en espacios'\n",
    "re.split(r'\\s+',texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Split', 'en', 'espacios']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texto.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cual es el patron?  \n",
    "A patron=r'\\s' <br>\n",
    "B patron=r'\\w' <br>\n",
    "C patron=r'[a-zA-Z]' <br>\n",
    "D patron=r'\\w+' <br>\n",
    "E ninguna de las anteriores <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['V', 'R', 'E']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frase = \"Vamos a implementar RegEx!\"\n",
    "re.findall(r'[A-Z]', frase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Vamos', 'a', 'implementar', 'RegEx']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frase = \"Vamos a implementar RegEx!\"\n",
    "re.findall(r'\\w+', frase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practiquemos con re.split y re.findall "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "string='hola! Como estas? Bien,gracias. que vas a hacer el Finde?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejercicio:\n",
    "\n",
    "Hacer una expresion regular para dividir el string en las respectivas frases\n",
    "noten que al final esta el signo de pregunta\n",
    "\n",
    "['hola', 'Como estas', 'Bien,gracias', 'que vas a hacer el Finde?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hola! Como estas? Bien,gracias. que vas a hacer el Finde?']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patron = r'\\w \\s'\n",
    "re.split(patron,string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Como', 'Bien', 'Finde']\n"
     ]
    }
   ],
   "source": [
    "#Vamos a encontrar todas las palabras que inicien con mayusculas\n",
    "patron = r\"[A-Z]\\w+\"\n",
    "print(re.findall(patron, string))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Tokenization\n",
    "\n",
    "Es la tarea de dividir un string en partes mas pequenas, por ejemplo: oraciones o\n",
    "palabras. Qué hicimos lineas arriba?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print(word_tokenize(\"hola, hay alguien ahi?\"))\n",
    "\n",
    "ejemplo=\"I don't like Pineda's shoes\"\n",
    "\n",
    "word_tokenize(ejemplo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### otros tokenizadores\n",
    "\n",
    "* sent_tokenize\n",
    "* regex_tokenize\n",
    "* TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "BASE_URL = \"https://archive.org/stream/JoseSaramagoEnsayoSobreLaCeguera/\\\n",
    "jose+saramago_ensayo+sobre+la+ceguera_djvu.txt\"\n",
    "\n",
    "response = requests.get(BASE_URL)\n",
    "data=response.text\n",
    "soup=BeautifulSoup(data, \"lxml\")\n",
    "soup.title\n",
    "libroSaramago=soup.body.div.pre.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![HEADER](IMAGENES/Seleccion_859.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragmento=libroSaramago[1000:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importar los modulos necesarios\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split el fragmento en sentencias\n",
    "sentences = sent_tokenize(fragmento)\n",
    "print(sentences)\n",
    "#tokenizar la sentencia 4 en palabras\n",
    "tokenized_words = word_tokenize(sentences[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sacar los tokens unicos del libro\n",
    "unique_tokens = list(set(word_tokenize(libroSaramago)))\n",
    "print(len(unique_tokens))\n",
    "unique_tokens[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#busquemos fechas\n",
    "patron1 = r\"[\\w\\s]+\\(\\d+\\)\"\n",
    "#print(re.search(patron1, libroSaramago))\n",
    "print(re.findall(patron1, libroSaramago))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![HEADER](IMAGENES/Seleccion_859.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identifiquemos quien habla\n",
    "string1='KING ARTHUR: Whoa there!  [clop clop clop] '\n",
    "patron2 = r\"[\\w\\s]+:\"\n",
    "print(re.match(patron2, string1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizar Tweets\n",
    "Cual patron eligen si quieren tokenizar pero consevar el #1 en un solo token?\n",
    "\n",
    "A r\"\\w+(\\?!)\" <br>\n",
    "B r\"(\\w+|#\\d|\\?|!)\" <br>\n",
    "C r\"(#\\d\\w+\\?!)\" <br>\n",
    "D r\"\\s+\" <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "string2 = \"SOLDIER #1: Found them? In Mercea? The coconut's tropical!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexp_tokenize(string2,???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "regexp_tokenize(string2,r\"(\\w+|#\\d|\\?|!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![HEADER](IMAGENES/Seleccion_859.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Usemos lo aprendido arriba para tokenizar tweets \n",
    "#saquemos el hashtag #therapyfail\n",
    "string3 =\"@machineplay I'm so sorry you're having to go through \\\n",
    "this. Again.  #therapyfail\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "qué patron debemos usar si queremos tokenizar y obtener la siguiente salida\n",
    "['@machineplay']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(regexp_tokenize(string3,patron3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![HEADER](IMAGENES/Seleccion_859.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NORMALIZAR TIPOS DE LETRAS\n",
    "\n",
    "Tipicamente utilizado, reduce la dimensión de los arreglos que veremos más adelante, pero\n",
    "se pueden perder algunas distinciones, por ejemplo: Apple y apple, la empresa y la fruta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carguemos los datos de la Metamorfosis de Kafka\n",
    "filename = 'metamorphosis_limpios.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "# split por espacios en blanco\n",
    "words = text.split()\n",
    "# convertir a minusculas\n",
    "words = [word.lower() for word in words]\n",
    "print(words[:100])\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# split en palabras\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens[:100])\n",
    "\n",
    "#veamos que What’s queda como What y ’s\n",
    "#Quedaron signos de puntuacion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![HEADER](IMAGENES/Seleccion_859.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FILTRAR SIGNOS DE PUNTUACIÓN\n",
    "Podemos filtrar lo que no nos interesa, por ejemplo signos de puntuacion aislados iterando\n",
    "sobre los tokens y solo dejar los tokens que son alfabeticos con la funcion isalpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words1 = [word for word in tokens if word.isalpha()]\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FILTRAR PALABRAS QUE NOS INTERESAN (STOP WORDS)\n",
    "\n",
    "Las stop words son las palabras que no contribuyen al significado profundo de la frase,\n",
    "son palabras como: the, a, is.\n",
    "Para algunos problemas como los de clasificacion de textos tiene sentido remover algunas\n",
    "palabras.\n",
    "NLTK tiene una lista de stop words en varios idiomas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)\n",
    "print(\"\\n\")\n",
    "print(words1[:100])\n",
    "print(\"\\n\")\n",
    "words = [w for w in words1 if not w in stop_words]\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![HEADER](IMAGENES/Seleccion_859.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEM O LEMATIZAR \n",
    "\n",
    "Es el proceso de reducir palabras hasta su raiz\n",
    "\n",
    "Algunas aplicaciones como por ejemplo la clasificacion de documentos se benefician de este\n",
    "tipo de prepocesamiento debido a que se reducen el vocabulario por ende la dimensionalidad \n",
    "y nos podemos enfocar en el sentimiento del documento en vez de profundizar en el significado\n",
    "\n",
    "Hay muchos algoritmos de stemming, pero el mas popular es Porter Stemming.\n",
    "Vamos a usarlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "filename = 'metamorphosis_limpios.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "# tokenizamos\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# pasamos por el lematizador\n",
    "porter = PorterStemmer()\n",
    "stemmed = [porter.stem(word) for word in tokens]\n",
    "print(stemmed[:100])\n",
    "\n",
    "#veamos que por ejemplo trouble queda troubl, tambien veamos que stem deja los token \n",
    "#en minuscula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![HEADER](IMAGENES/Seleccion_859.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONSIDERACIONES ADICIONALES\n",
    "\n",
    "Esto que hemos visto es solo el inicio, el texto con el que trabajamos estaba relativamente\n",
    "limpio, sin embargo en sus proyectos se pueden encontrar con otros problemas, como:\n",
    "    * Trabajar con documentos grandes que no pueden ser guardados en RAM - Hadoop\n",
    "    * Extraer el text de un HTML - Scrapping, regex\n",
    "    * trabajar con diversos lenguajes - import polyglot\n",
    "    * Trabajar con palabras de una rama del conocimiento especifica, frases o acronimos\n",
    "    * manejar numeros como fechas y cantidades\n",
    "    * manejar errores frecuentes de escritura\n",
    "    * etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![HEADER](IMAGENES/Seleccion_859.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Un poco de analisis exploratorio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tablagoogle](IMAGENES/Seleccion_865.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "plt.hist([1, 2,2,2,2, 7, 7, 7, 9])\n",
    "plt.show()\n",
    "\n",
    "palabrasCegera=word_tokenize(libroSaramago)\n",
    "lenwords=[len(w) for w in palabrasCegera]\n",
    "plt.hist(lenwords)\n",
    "plt.show()\n",
    "\n",
    "#cual es la palabra mas larga\n",
    "import numpy as np\n",
    "arraypalabrasCegera=np.asarray(palabrasCegera)\n",
    "arraylenwords=np.asarray(lenwords)\n",
    "arraypalabrasCegera[arraylenwords==np.max(arraylenwords)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![HEADER](IMAGENES/Seleccion_859.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRACTICA DE NLP, REGEX Y ANALISIS EXPLORATORIO\n",
    "\n",
    "Vamos a elegir un libro del proyecto Gutenberg y vamos a determinar la palabra que más se repite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En __[GUTENBERG](http://www.gutenberg.org5)__ vamos a seleccionar nuestro clásico favorito y vamos a identificar la palabra que más se repite\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![HEADER](IMAGENES/Seleccion_866.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El principe feliz de Wide\n",
    "r =requests.get(\"http://www.gutenberg.org/files/902/902-h/902-h.htm\")\n",
    "r.encoding = 'utf-8'\n",
    "\n",
    "\n",
    "html = r.text\n",
    "html[:2000]\n",
    "\n",
    "#Extraer el texto del HTML\n",
    "#soup = BeautifulSoup(html, \"lxml\")\n",
    "soup = BeautifulSoup(html,\"html5lib\")\n",
    "text = soup.get_text()\n",
    "text[50000:60000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizemos las palabras usando regexp_tokenize\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "#patron=?\n",
    "tokens=regexp_tokenize(text,r'\\w+')\n",
    "print(tokens[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargar stop words en ingles y retirarlas del texto\n",
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words('english')\n",
    "#sw = topwords.words('spanish')\n",
    "print(sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![HEADER](IMAGENES/Seleccion_859.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convertir las palabras a minusculas metodo lower()\n",
    "words = [t.lower() for t in tokens]\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargar stop words en ingles y retirarlas del texto\n",
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words('english')\n",
    "#sw = topwords.words('spanish')\n",
    "print(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_ns = [w for w in words if not w in sw]\n",
    "words_ns[:5]\n",
    "\n",
    "#Grafiquemos el top de las 100 palabras mas usadas\n",
    "\n",
    "freqdist = nltk.FreqDist(words_ns)\n",
    "\n",
    "# Plotting the word frequency distribution\n",
    "freqdist.plot(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![HEADER](IMAGENES/Seleccion_859.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMÓ PREPARAR LOS DATOS USANDO SKLEARN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los textos requieren una preparación especial antes de ser usados en un modelo predictivo\n",
    "las palabras deben ser codificadas a numeros ya sean enteros o reales, para ser usadas en\n",
    "un algoritmo de machine learning, este proceso es llamado extracción de caracteristicas o\n",
    "descriptores (vectorización).\n",
    "\n",
    "SKLEARN ofrece unas herramientas muy poderosas y faciles de usar para realiza la\n",
    "tokenizacion y la extraccion de caracteristicas.\n",
    "\n",
    "A continuación, vamos a ver como preparar los datos para un modelo predictivo usando SKLEARN\n",
    "\n",
    "Vamos a ver como:\n",
    "    * convertir texto en un vector de conteos con \"CountVectorizer\"\n",
    "    * convertir texto en un vector de frecuencia con \"TfidfVectorizer\"\n",
    "    * convertir un texto en enteros unicos usando \"HashingVectorizer\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![HEADER](IMAGENES/Seleccion_859.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "string4=\"\"\"The cat is in the box. The cat likes the box. The box is over the cat.\"\"\"\n",
    "                 \n",
    "tokens_string4=word_tokenize(string4)\n",
    "counter=Counter(tokens_string4)\n",
    "counter.most_common(2)\n",
    "\n",
    "lower_tokens4=[t.lower() for t in tokens_string4 if t.isalpha()]\n",
    "counterl=Counter(lower_tokens4)\n",
    "counterl.most_common(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio: usar stopwords e imprimir las 5 palabras más comunes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![HEADER](IMAGENES/Seleccion_859.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer SKLEARN\n",
    "Provee una forma de vectorizar nuestra colección de documentos y construir un vocabulario\n",
    "de palabras conocidas, también permite codificar nuevos documentos usando el vocabulario\n",
    "conocido (entrenamiento y test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importamos CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Hacemos una lista de documentos - textos -> Algun Dataset\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\",\n",
    "        \"The quick brown fox jumped over the lazy cat.\"]\n",
    "# creamos una instancia\n",
    "vectorizer = CountVectorizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizamos y construimos el vector de vocabulario\n",
    "vectorizer.fit(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploremos el vector, accediendo a la codificación del vocabulario\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# codificar el documento\n",
    "vector = vectorizer.transform(text)\n",
    "\n",
    "# exploremos los documentos codificados\n",
    "\n",
    "print(vector.shape)\n",
    "print(type(vector))\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![HEADER](IMAGENES/Seleccion_859.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que se retorna un vector con la logitud del vocabulario encontrado y un entero que cuenta\n",
    "el número de veces que aparece cada palabra en el documento. Como el vector puede contener muchos\n",
    "ceros, entonces es retornado en formato de matriz dispersa (scipy provee una forma eficiente de\n",
    "trabajar con matrices dispersas -scipy.sparse). Transformamos la matriz dispersa a Numpy arrays\n",
    "para tener un mejor entendimiento de la forma como salen los resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que CountVectorizer también tokeniza el documento, convierte todas las palabras a minusculas\n",
    "e ignora los signos de puntuación, estos aspectos pueden ser configurados en las opciones de \n",
    "CountVectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Miremos que pasa si incluimos un vector con palabras que no estaban en los documentos de entrada**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = [\"the puppy\"]\n",
    "vector = vectorizer.transform(text2)\n",
    "print(vector.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La salida del CountVectorizer puede ser usada como entrada para un algoritmo de ML\n",
    "como la salida es un enterio hay algortmos de ML como Naive Bayes que tienen mejor rendimiento\n",
    "con este tipo de datos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![HEADER](IMAGENES/Seleccion_859.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FRECUENCIA DE PALABRAS CON TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contar las palabras es un buen inicio, pero es muy básico. Uno de los problemas por \n",
    "ejemplo que palabras como \"the\" que aparecen mucho en el documento, entonces, tendrán \n",
    "un conteo grande cuando en realidad no son tan importantes para el significado del documento.\n",
    "\n",
    "Una alternativa para superar este impase es calcular la frecuencia de las palabras, \n",
    "el método mas popular es TF-IDF. TF-IDF es el acrónimo para Term Frequency-inverse \n",
    "Document Frequency (ver teoría de la información) y da una idea de la importancia \n",
    "de cada palabra en el documento. (Ya vieron los conceptos teoricos Sebas Pineda)\n",
    "\n",
    "Term frequency: nos ilustra la frecuencia  con la que una palabra dada aparece dentro de un documento\n",
    "Inverse Document Frequency: Da un puntaje pequeño a las palabras que parecen mucho \n",
    "en un documento. Nuestra hipótesis es que la importancia viene acompañada de la escasez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# lista de documentos\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\",\n",
    "\t\t\"The dog.\",\n",
    "\t\t\"The fox\",\n",
    "        \"The cat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intanciemos el objeto\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.fit(text)\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectorizer.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# codificamos el documento\n",
    "vector = vectorizer.transform([text[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspeccionemos los objetos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![HEADER](IMAGENES/Seleccion_859.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HashingVectorizer - Ver referencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UNA FORMA DE REDUCIR LA DIMENSION DEL PROBLEMA\n",
    "\n",
    "\n",
    "Conteos (CountVectorizer) y frecuencias (TfidfVectorizer) son muy utiles, pero tiene una\n",
    "limitación y es que el vocabulario se puede volver muy grande. Esto se traduce a vectores\n",
    "de codificación muy grandes y estos podrían requerir bastante memoria además de hacer más\n",
    "lenta la ejecución de los algoritmos.\n",
    "\n",
    "un trabajo inteligente es usar un hash de palabras de una sola dirección para convertirlas \n",
    "en enteros. La parte inteligente es que no se requiere vocabulario y puedes elegir un vector\n",
    "de longitud fija arbitrariamente larga.\n",
    "\n",
    "Una desventaja es que el HASHING es una función sin inversa, esto significa que una vez\n",
    "codificado un texto este no puede regresarse. Sin embargo esto no importa en la mayoria\n",
    "de algoritmos de ML.\n",
    "\n",
    "El reto es elegir el numero de caracteristicas para minimizar la probabilidad de colision pero\n",
    "buscando tambien reducir la dimensionalidad, hay heuristicas para estimar el numero de\n",
    "caracteristicas\n",
    "\n",
    "\n",
    "Notemos que este vectorizador no requiere entrenamiento con los documentos. Despues de\n",
    "instanciarlo se puede usar directamente sobre cualquier documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
    "# create the transform\n",
    "vectorizer = HashingVectorizer(n_features=20)\n",
    "# encode document\n",
    "vector = vectorizer.transform(text)\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algunas referencias\n",
    "\n",
    "CountVectorizer scikit-learn API. <br>\n",
    "    http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html <br>\n",
    "TfidfVectorizer scikit-learn API. <br>\n",
    "    http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html <br>\n",
    "TfidfTransformer scikit-learn API. <br>\n",
    "    http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html <br>\n",
    "HashingVectorizer scikit-learn API. <br>\n",
    "    http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html <br>\n",
    "\n",
    "Esto es solo una pincelada, creo que vale la pena explorar con más profundidad tanto desde\n",
    "lo teorico como desde lo práctico estos metodos que hemos visto.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![HEADER](IMAGENES/Seleccion_859.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clasificacion de noticias falsas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hideOutput": false,
    "hidePrompt": false
   },
   "source": [
    "### Base de datos de entrenamiento y prueba\n",
    "\n",
    "En ML nuestros modelos son tan buenos como lo que le entramos para entrenarlos, si entra basura, entonces, saldra basura.\n",
    "\n",
    "Tambien es importante la cantidad de ejemplos de nuestra base de datos, mientras mas ejemplos tenemos mas posibilidades de **generalizar**.\n",
    "\n",
    "Debemos buscar una **base de datos balanceada** o de los contrario nuestro modelos podrian tener sesgos hacia una clase determinada.\n",
    "\n",
    "![sesgos1](IMAGENES/Seleccion_828.png)\n",
    "\n",
    "![sesgos2](IMAGENES/Seleccion_829.png)\n",
    "\n",
    "[sesgos2](IMAGENES/Seleccion_830.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![HEADER](IMAGENES/Seleccion_859.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('fake_or_real_news.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.label\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.text,y,test_size=0.30, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciar CountVectorizer\n",
    "count_vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "# Entrenar el countvectorizer y transformar la matriz de entrenamiento \n",
    "count_train = count_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# transformar la matriz de prueba\n",
    "count_test = count_vectorizer.transform(X_test)\n",
    "\n",
    "# imprimir los 10 primeros descriptores\n",
    "print(count_vectorizer.get_feature_names()[:10])\n",
    "\n",
    "#Veamos la matriz dispersa\n",
    "print(count_train.A[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![HEADER](IMAGENES/Seleccion_859.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instanciar el vectorizador tfidf\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "\n",
    "#  Entrenar el tfidfvectorizer y transformar la matriz de entrenamiento  \n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# transformar la matriz de prueba \n",
    "tfidf_test = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# imprimir los 10 primeros descriptores\n",
    "print(tfidf_vectorizer.get_feature_names()[:10])\n",
    "\n",
    "#Veamos la matriz dispersa\n",
    "print(tfidf_train.A[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio:\n",
    "\n",
    "Con los modelos vistos en reuniones anteriores van a entrenar 3 clasificadores con las matrices de descriptores construidas en los puntos anteriores.\n",
    "También van a estimar los mejores hiperparametros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![HEADER](IMAGENES/Seleccion_859.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimacion manual de hiperparametros\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAREA 1\n",
    "## PRACTIQUEMOS LO QUE HEMOS VISTO CONSTRUYENDO UNA PIPELINE\n",
    "\n",
    "* Vamos a cargar el texto XXX\n",
    "* Lo vamos a dividir en tokens\n",
    "* lo vamos a convertir a minusculas o mayusculas (lower o upper case)\n",
    "* vamos a remover los signos de puntuacion para cada token\n",
    "* luego vamos a remover los tokens que no son alfabeticos\n",
    "* vamos a crear nuestra lista de stop words y luego eliminar las stop words que nuestro texto\n",
    "* al final vamosa retornar el texto limpio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploremos la base de datos Sentimient140 de la universidad Stanford\n",
    "\n",
    "__[Enlace de descarga de la base de datos de entrenamiento y prueba](http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip)__\n",
    "    \n",
    "__[Paper en el que se explica como fue construida la base de datos](https://cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf)__\n",
    "\n",
    "A continuacion vamos a explorar la base de datos\n",
    "\n",
    "![sesgos2](IMAGENES/Seleccion_831.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![HEADER](IMAGENES/Seleccion_859.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "\n",
    "colums = ['SENTIMIENTO','ID','FECHA','QUERY','USUARIO','TEXTO']\n",
    "df = pd.read_csv(\"stanford_emotion_dataset/training.1600000.processed.noemoticon.csv\",\n",
    "                 header=None, names=colums,encoding = \"ISO-8859-1\")\n",
    "#tambien se pudo haber usado el alias de ISO-8859-1, latin1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imprimir los 5 primeros registros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vamos a hacer una revision superficial de nuestra base de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"SENTIMIENTO\"]==4].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![HEADER](IMAGENES/Seleccion_859.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[df[\"SENTIMIENTO\"]==4].TEXTO.iloc[1])\n",
    "print(len(df[df[\"SENTIMIENTO\"]==4].TEXTO.iloc[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[df[\"SENTIMIENTO\"]==0].TEXTO.iloc[1])\n",
    "print(len(df[df[\"SENTIMIENTO\"]==0].TEXTO.iloc[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retiremos la informacion innecesaria para nuestro problema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hay NaNs?\n",
    "\n",
    "> *decir si hay nan, en caso de que si eliminarlos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![HEADER](IMAGENES/Seleccion_859.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vamos a revisar si la base de datos esta balanceada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escribir el codigo para ver cuantos registros hay de cada clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longitudtweets=[len(i) for i in df[\"TEXTO\"]]\n",
    "len(longitudtweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graficar el histograma de la longitud de caracteres por tweet. En el eje X numero de caracteres, en el eje y cantidad de tweets con dicho numero de caracteres.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist <br>\n",
    "rwidth <br>\n",
    "range <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideOutput": false
   },
   "outputs": [],
   "source": [
    "df[\"LOGITUD\"]=longitudtweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideOutput": false
   },
   "outputs": [],
   "source": [
    "list(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![HEADER](IMAGENES/Seleccion_859.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![peligro](IMAGENES/Seleccion_839.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graficar los 20 primeros registros cuya longitud de caracteres es superior a 140"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuantos tweets hay con longitud de caracteres por encima de 140"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![HEADER](IMAGENES/Seleccion_859.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "prueba = BeautifulSoup(df.TEXTO[747], 'lxml')\n",
    "print (prueba.get_text())\n",
    "print(len(prueba.get_text()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "prueba = BeautifulSoup(df.TEXTO[492], 'lxml')\n",
    "print (prueba.get_text())\n",
    "print(len(prueba.get_text()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideOutput": false
   },
   "outputs": [],
   "source": [
    "# Las paginas web no nos dan informacion sobre los sentimientos y las menciones tampoco\n",
    "df.TEXTO[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hacer una expresión regular y usar la función re.sub para limpiar paginas web: http, https y www. \n",
    "Probar con el ejemplo anterior.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hacer una expresión regular y usar la función re.sub para limpiar las menciones. Probar con el ejemplo anterior.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![HEADER](IMAGENES/Seleccion_859.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.TEXTO[5204]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.sub(r\"[^a-zA-Z]\", \" \", df.TEXTO[5204])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correccion de errores ortograficos (No se usará en este ejecicio)\n",
    "\n",
    "Las siguientes funciones van de cuenta de Peter Norvig\n",
    "\n",
    "http://norvig.com/spell-correct.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "WORDS = Counter(words(open('big.txt').read()))\n",
    "\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![HEADER](IMAGENES/Seleccion_859.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TAREA:\n",
    "Van a hacer una función que reciba de entrada un tweet sucio y lo limpie con todo lo que hemos visto en los puntos anteriores. <br>\n",
    "Van a ingresar todos los tweets limpios en una lista que posteriormente agregaran al dataframe con el nombre de:\n",
    "\"TEXTO_LIMPIO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"TEXTO_LIMPIO\"]=docsclean\n",
    "df[\"LOGITUD_TEXTO_LIMPIO\"]=lendocsclean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.TEXTO[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideOutput": false
   },
   "outputs": [],
   "source": [
    "print(df.TEXTO_LIMPIO.iloc[0])\n",
    "print(len(docsclean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Graficar el histograma con el texto limpio**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Van a entrenar 3 modelos usando CountVectorizer y TfidfVectorizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encontremos el mejor hiperparametro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "for alfa in np.arange(0.1,1,step=0.1):\n",
    "    print('Alfa: ', alfa)\n",
    "    print('Score: ', train_and_predict(count_train,y_train,count_test,y_test,alfa))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "for alfa in np.arange(0.1,1,step=0.1):\n",
    "    print('Alfa: ', alfa)\n",
    "    print('Score: ', train_and_predict(tfidf_train,y_train,tfidf_test,y_test,alfa))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
